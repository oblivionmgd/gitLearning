# 深層強化学習の初心者向けガイド
## 冒頭
### 単語
- determinate 
	- 判断する
- ought to
	- ~なければならない
- accordance
	- 軌を一に
- probable
	- 有り得る
 
### デカルトの言葉
 何が真実かを判断する力がないときは、最も可能性の高いものに従って行動すべきである
 
 
## Introduction
### 単語
- reinforcement
	- 補強、強化
- combines
	- 合体
- agent
	- 代理人
- attain
	- 得る 
- expect
	- 期待する
- recent
	- 最近の
- astounding
	- 驚くべき、驚異的
- prediction
	- 予測
- refers to 
	- を指す
- orient
	- 志向
- incorporate
	- 取り入れる
- beat
	- 打ち負かす
- numerous
	- 数多くの
- mapping
	- 経路を確定する

### 概要
　深層強化学習とは人工知能と強化学習を合体させ、ソフトウェアが目標を達成するために仮想環境で可能な限り最善の行動を学習することを可能にする。つまり、関数近似と目標最適化を合成し、状態と行動をペアとして期待される報酬のためにマッピングをする

　できる限り優しい言葉を使いながら解説をする
　
　ニューラルネットワークは、コンピュータビジョン、機械学習、そして機械翻訳と時系列予測などの問題における、最近のAIブレイクスルーを担っているが、それらもまた強化学習アルゴリズムと組み合わせることで、囲碁で世界チャンピオンとなったDeepmind's AlphaGoのような驚異的なものを生み出すことができる
　
　強化学習とは目標志向のアルゴリズムを指し、複雑な目的（目標）を達成する方法や、多くのステップにわたって特定の次元に沿って最大化する方法を学習します。RLアルゴリズムは何もない状態からスタートすることができ、適切な条件下では、それらは超人的なパフォーマンスを示します。叱ったり、おやつを与えたりすることでインセンティブを得られるペットのように、これらのアルゴリズムも誤った判断をしたときはペナルティを与え、正しい判断をした場合は報酬を与えます。これが強化です。
　
　深層ニューラルネットワークを取り入れた強化アルゴリズムは、アタリのビデオゲーム, Starcraft ll and Dota-2、囲碁の世界チャンピオンのようなゲームをプレイしている数多くのエキスパートに対しても勝つことができます。非ゲーマには些細なことに聞こえるかも知れませんが、これまでの功績より大きく強化学習が改善したものであり、技術の最先端は急速に成長しています。
　
　強化学習は、即時の行動とそれらが生み出した遅延があるリターンとの関係性という難しい問題を解決します。人間のように、強化学習アルゴリズムは時々、結果を見るための決断のために待つ必要があります。強化学習アルゴリズムは、多くの時間ステップでどの行動がどの結果につながるのか理解することが困難な、遅延リターン環境で動作します。
　
　強化学習アルゴリズムが再現性のあるビデオゲームの限られた選択肢からではなく任意の数の可能な施工動向から選択しながら、よりあいまいな実生活でゆっくりとより良いパフォーマンスを発揮するのと改定するのは合理的である。つまり、時間が経てば現実世界での目標を達成するために価値のあるものになると期待している。十分なデータと計算量がアレば強力なAIへの最も有望な道になるかも知れません。
　
　Pathmindは、深層強化学習を実世界のシミュレーションに適用し、工場建築、スタッフコールセンター、倉庫やサプライチェーンの設置、流通経路の管理などを企業が最適化できるように支援します。
　
　
### Reinforcement Learning Definitions (強化学習の定義)

### 単語
- tend
	- 傾く、傾向
- denote
	- 表す

### 訳

　強化学習はエージェント、環境、状態、行動、そして報酬の概念を使って理解することができます。
　
　大文字は物事の集合を表すことに使われる傾向があり、小文字はその物事の特定のインスタンスを表します。
　
　エージェント：エージェントはドローンが配達したり、マリオがビデオゲームの案内をしたりと行動を起こします。そのアルゴリズムがエージェントです。それはその生活のなかであなたであることを考慮するのが役に立つかも知れません。
　
　アクション：Aは、エージェントが行うことができるすべての可能な動きの集合です。アクションはほとんど自明のことですが、エーゲンとは通常、離散的で可能なアクションのリストから選択することにちゅうしてください。
　
　割引率：割引係数エージェントの行動選択に対するこれらの報酬の効果を減衰させるために、エージェントによって発見されたように将来の報酬によって乗算される。
　
　環境：エージェントが移動し、エージェントが応答する世界。その環境は、エージェントの現在の状態とアクションを入力として受け取り、応答をエージェントの報酬をアウトプットとして返す。そして次の状態へ移行する。あなたがエージェントである場合、その環境はあなたの行動を処理し、その結果を決定する物理学の法則や、社会のルールである可能性がある。
　
　状態：状態とはエージェントが自分自身で見つける具体的で即時的な状態な状況のことです。
　
　報酬：エージェントのアクションより受け取ったより成功か失敗かを判断するためのフィードバックである。例えば、ビデオゲームでは、マリオがコインに触れたた時、彼はポイントを得ます。与えられた状態から、エージェントは行動の形で環境に出力を贈り、環境はエージェントの新しい状態と報酬があればそれを返す。報酬は即時でも遅延であっても良い。それらは効果的にエージェントの行動を評価します。
　
　
　ポリシー：ポリシーは現在の状態に基づいて次の行動を決定するためにエージェントが採用する戦略です。それは状態を行動にマップし、その行動を最高の報酬を約束する行動を行います。
　
　
　価値：政策のもとで現在の状態の長期的な期待収益として定義される。我々は、報酬を割引するか、またはその推定値を低くして、それが起こるよりも先の未来になるほど割引する。
　
　Q値：現在の状態ｓから政策
　
　軌跡：それらの状態に影響を与える状態シーケンスと行動。
　ラテン語の横断するから。エージェントの人生には、
　
　主要な区別：報酬は与えられた状態で受け取る即時の信号であり、一方その状態から期待するかも知れない全ての報酬の総和である。価値は長期的な期待であり、報酬は即時の喜びである。価値とは、長く健康な人生を期待してほうれん草のサラダを食べることであり、報酬とは夕食にコカインを食べて地獄に落ちることである。
　これらは時間軸が異なる。だからあなたは価値と報酬が別れた状態を持つことができる。あなたは長期的な勝ちがある可能性の高いポジションに移っても、低い即効性のある報酬を受け取るかも知れないし、高い即効性のある報酬を受け取るかも知れないが、それは時間の経過とともに展望が薄れていくことになる。このような理由から、強化学習が予測しコントロールしようとしているのは、即時報酬ではなく価値観数である。
　
　
　つまり環境とは、現在状態で行われた行動を次の状態と報酬に変換する機能である。また、エージェントは機能を新しい状態と報酬を次の行動に変換する機能である。私達はエージェントの機能を知って設定することができますが、強化学習を応用して便利で面白いと思うと思う場面では、環境の機能を知らないことがほとんどです。それは私達が入力と出力しか見ていないブラックボックスです。それは殆どの人々と技術との関係のようなもので、私達はそれが何をするのかを知っていますが、それがどのようにして動くのかは知りません。強化学習は、それが吐き出す報酬を最大化するブラックボックス環境に行動を送ることができるように、環境の関数を禁じさせようとするエージェントの試みを表しています。
　
　
　上記のフィードバックループでは、添字が時間ステップtとt+1を示し、それぞれが異なる状態を表しています。t時の状態とt+1時の状態です。他の教師あり学習や教師なし学習とは異なり、強化学習は、次から次へと発生する状態と行為のペアという観点からのみ、順次に考えることができます。
　
　強化学習は行動をその結果によって判断する。それは目標志向であり、その目的はエージェントを目標達成に導く、あるいは目的関数を最大化するような一連の行動を学習することにある。いくつかの例を紹介します。
　
　- ビデオゲームにおいて、目標はゲームを高い点で終えることであり、獲得したポイントはエージェントのその後の行動に影響を与えます。すなわち、エージェントはスコアを最大化するために戦艦をうち、コインをタッチし、流星をかわすべきであることを学習するかも知れません。

　- 現実世界では、その目標はロボットがA地点からB地点まで移動することが目的であり、ロボットが地点Bに近づくごとに１インチごとにポイントのようにカウントされるかも知れません。

　ここに強化学習の目的関数の例があります。
　
　私達は報酬関数を表すｒと時間ステップを表すｔを合計しています。したがって、この目的関数はすべての例えばすべての
　　